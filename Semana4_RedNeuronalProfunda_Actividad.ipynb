{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIIA-4203 MODELOS AVANZADOS PARA ANÁLISIS DE DATOS II\n",
    "\n",
    "\n",
    "# Red neuronal multi-capa\n",
    "\n",
    "## Actividad 4\n",
    "\n",
    "### Profesor: Camilo Franco (c.franco31@uniandes.edu.co)\n",
    "\n",
    "En este cuadernos estudiaremos algunos conceptos claves del aprendizaje profundo, y construiremos paso a paso una red densamente conectada con múltiples capas escondidas. Implementaremos la red profunda repasando conceptos de optmización e inicialización de pesos. Finalmente probaremos nuestra red para el tratamiento de imagenes e intentar mejorar los resultados que obtuvimos con nuestra red sencilla para la detección automática de frailejones sore imagenes aereas del páramo. \n",
    "\n",
    "Estas actividades para la construcción de redes neuronales desde cero se han basado en:\n",
    "http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombres:\n",
    "\n",
    "\n",
    "                         Names                 User                Code\n",
    "           - Romero Fonseca Angela Fernanda       af.romerof     201819276       \n",
    "           - Juan Fernando Ortiz Serrano          jf.ortiz43     200623117\n",
    "           - Elquin Huertas Ramírez               e.huertas      201920061"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Previamente hemos entrenado una red neuronal sencilla de dos capas (con una capa escondida y una capa de salida). Ahora vamos a construir una red profunda, con múltiples capas escondidas. Primero vamos a implementar las funciones requeridas para construir una red neuronal profunda. \n",
    "\n",
    "Utilizaremos unidades no-lineales mediante una función como la *Rectified Linear Unit* (ReLU), la cual permite disminuir los tiempos de entrenamiento de una red compleja. Implementaremos diferentes técnicas de inicialización de parámetros, como la incialización de *Xavier* o de *He* (K. He, X. Zhang, Sh. Ren, J.Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. Proceedings IEEE International Conference on Computer Vision (ICCV), 2015, 1026-1034).\n",
    "\n",
    "Finalmente implementaremos nuestro código para clasificación de imagenes aereas del paramo y la detección automática de plantas nativas.\n",
    "\n",
    "Para la construcción de la red, denotaremos por el superíndice $[l]$ a los parámetros de la *l-ésima* capa. Por ejemplo, $a^{[l]}$ corresponderá con la activación de la *l-ésima* capa, y $W^{[l]}$ y $b^{[l]}$ serán los parámetros de la *l-ésima* capa.\n",
    "\n",
    "El superíndice $(i)$ denota una cantidad asociada con el *i-ésimo* ejemplo. Así, $x^{(i)}$ será el *i-ésimo* ejemplo de entrenamiento. Y el subíndice $i$ denota la *i-ésima* entrada de un vector. De esta manera $a^{[l]}_i$ denota la *i-ésima* entrada de las activaciones de la *l-ésima* capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importemos algunos de los paquetes que vamos a utilizar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ImportImagenes_L import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # se fija el tamaño de los gráficos\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para construir la red neuronal, primero se construirán funciones auxiliares que permitirán implementar una red neuronal de L capas. Primero estudiaremos la incializacion de los parametros para una red de $L$ capas, e implementaremos la propagación hacia delante.\n",
    "\n",
    "La arquitectura de la red consistirá en capas densamente conectadas que propagan hacia delante el mensaje LINEAL de las capas anteriores, obteniendo $Z^{[l]}$, y activandose mediante una función de ACTIVACION, **RELU** o **SIGMOIDE**. \n",
    "\n",
    "De esta manera, se combinan los pasos [LINEAL->ACTIVACION] en una sola función (hacia delante), y tras ello, se agrupan las funciones de propagación hacia delante [LINEAL->RELU] L-1 veces (para las capas 1 hasta L-1). Finalmente se añade la capa de salida [LINEAL->SIGMOIDE] para la última capa $L$. Esta agrupacion la tendremos en la función ``propaga_L``.\n",
    "\n",
    "A cada función de propagación hacia delante le corresponde una función hacia atrás. Por ello, a cada paso hacia delante, guardaremos algunos valores necesarios para calcular los gradientes.\n",
    "\n",
    "Luego compytaremos la pérdida e implmentaremos la retro-propagación. Completaremos la parte LINEAL de la retro-propagación de una capa, a partir del gradiente de la función de activación, y combinaremos los pasos en una nueva función de retro-propagación [LINEAL->ACTIVACION]. Agruparemos las funciones de retro-propagación [LINEAL->RELU] L-1 veces y añadiremos la correspondiente [LINEAL->SIGMOIDE] en una nueva función ``retro_L``.\n",
    "\n",
    "Por último, actualizaremos los parámetros para implementar el bucle de aprendizaje de la red y ajustar el modelo de red con los datos que nos sean proporcionados.\n",
    "\n",
    "\n",
    "*Lectura opcional sobre funciones de activación tipo SELU: https://towardsdatascience.com/gentle-introduction-to-selus-b19943068cd9*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Inicialización de parámetros\n",
    "\n",
    "Entrenar una red neuronal requiere especificar unos valores iniciales para los pesos. Un método de inicialización bien elegido va a mejorar bastante el aprendizaje. La inicialización de una red neuronal profunda es más compleja que la de una red sencilla, pues contamos con una mayor número de parámetros (hay más matrices de pesos y vectores de sesgo).   \n",
    "\n",
    "Un método de inicialización bien elegido permite:\n",
    "- Incrementar la velocidad de convergencia de la búsqueda por los parámetros (mediante GD) \n",
    "- Incrementar las probabilidades de que el método de búsqueda (GD) converja hacia un error de entrenamiento menor, y una mejor generalización  \n",
    "\n",
    "La incialización la implementaremos mediante la fucnión `param_I_L`.\n",
    "\n",
    "Recuerde asegurarse que las dimensiones de las matrices con los parametros sean coherentes al pasar de capa en capa. Recuerde que  $n^{[l]}$ es el número de unidades en la capa $l$. Entonces, e.g., si el tamaño de la entrada $X$ es $(14700, 175)$ (con $m=175$ ejemplos), se tiene que:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td>       </td> \n",
    "        <td> **Dimensión de W**     </td> \n",
    "        <td> **Dimensión de b**   </td> \n",
    "        <td> **Activación**</td>\n",
    "        <td> **Dimensión de la activación**  </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Capa 1** </td> \n",
    "        <td> $(n^{[1]},14700)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},175)$ </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Capa 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 175)$ </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Capa L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 175)$ </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Capa L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 175)$  </td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1.1\n",
    "\n",
    "Implemente la inicialización de una red neuronal con L capas. \n",
    "\n",
    "Tenga en cuenta que:\n",
    "- Inicializaciones diferentes llevan a resultados distintos\n",
    "- En redes complejas, si los pesos son inicializados muy pequeñas o muy grandes, podemos encontrarnos con el problema de gradientes que se desvanecen o que explotan.\n",
    "- La incialización aleatoria es usada para romper la simetría y asegurarse que las distintas unidades escondidas (neuronas) puedan aprender funciones distintas\n",
    "\n",
    "Recuerde que la estructura del modelo es [LINEAL -> RELU] $ \\times$ (L-1) -> LINEAL -> SIGMOIDE. Esto es, la red tiene $L-1$ capas utilizando una función de activación ReLU, seguido de una capa de salida con la función de activación Sigmoide.\n",
    "\n",
    "i) Implemente una inicialización aleatoria para las matrices de pesos. \n",
    "Para ello utilice `np.random.randn(shape) * 0.01`.\n",
    "\n",
    "Para los sesgos, utilize una inicialización de ceros. \n",
    "Para ello utilice `np.zeros(shape)`.\n",
    "\n",
    "ii) Implemente la inicialización de Xavier y de He. El método de He fue presentado en el paper publicado por He et al., 2015. Es muy similar a la inicialización de Xavier, sólo que en la de Xavier los pesos  $W^{[l]}$ se multiplican por `sqrt(1./dim_capas[l])`, mientras que en la de He se multiplican por `sqrt(2./dim_capas[l])`.)\n",
    "\n",
    "*Nota:* la inicialización de He se recomienda para las capas con una activación ReLU. \n",
    "\n",
    "\n",
    "El número de unidades en cada capa $n^{[l]}$, se guarda en la variable `dim_capas`. De esta manera, por ejemplo, las `dim_capas` con 2 dimensiones de entrada, una capa escondida con 4 unidades, y una capa de salida con una unidad, corresponde con [2,4,1]. Por lo tanto, la forma de `W1` sería de (4,2), la de `b1` (4,1), `W2` (1,4) y `b2` (1,1). \n",
    "\n",
    "\n",
    "*Ayuda:* La implementación para $L=1$ sería de la siguiente manera:\n",
    "```python\n",
    "    if L == 1:\n",
    "        param[\"W\" + str(L)] = np.random.randn(dim_capas[1], dim_capas[0]) * 0.01\n",
    "        param[\"b\" + str(L)] = np.zeros((dim_capas[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_I_L(dim_capas, init, semilla):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dim_capas: arreglo (lista) de python con las dimensiones de cada capa de la red\n",
    "    init: método de incialización \"rand\", \"Xav, \"He\"\n",
    "    semilla: la semilla aleatoria para la incializacion\n",
    "    Output:\n",
    "    param: diccionario python con los parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl: matriz de pesos (dim_capas[l], dim_capas[l-1])\n",
    "                    bl: vector de sesgo (dim_capas[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed( semilla )\n",
    "    param = {}\n",
    "    L = len(dim_capas)           \n",
    "\n",
    "    for l in range(1, L):\n",
    "        \n",
    "        if init == \"rand\":\n",
    "        # Inicializacion aleatoria\n",
    "            param['W' + str(l)] = np.random.randn(dim_capas[l], dim_capas[l-1]) * 0.01\n",
    "            param['b' + str(l)] = np.zeros((dim_capas[l], 1))\n",
    "        \n",
    "        # Inicializacion de Xavier\n",
    "        #Inicializacion de He\n",
    "        elif init == \"Xav\":\n",
    "            param['W' + str(l)] = np.random.randn(dim_capas[l], dim_capas[l-1]) * np.sqrt(1./dim_capas[l])\n",
    "            param['b' + str(l)] = np.zeros((dim_capas[l], 1))\n",
    "            \n",
    "        #Inicializacion de He\n",
    "        elif init == \"He\":\n",
    "            param['W' + str(l)] = np.random.randn(dim_capas[l], dim_capas[l-1]) * np.sqrt(2./dim_capas[l-1])\n",
    "            param['b' + str(l)] = np.zeros((dim_capas[l], 1))\n",
    "            \n",
    "        \n",
    "        assert(param['W' + str(l)].shape == (dim_capas[l], dim_capas[l-1]))\n",
    "        assert(param['b' + str(l)].shape == (dim_capas[l], 1))\n",
    "\n",
    "        \n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializacion aleatoria:\n",
      "W1 = [[ 1.10855471e-05 -2.89544069e-03 -1.11606630e-02 -1.28827567e-04\n",
      "  -3.78361464e-03]\n",
      " [-4.81135363e-03 -1.51733118e-02 -4.90871981e-03 -2.40680579e-03\n",
      "  -6.47947460e-03]\n",
      " [ 6.35891080e-03  1.74011731e-02  2.96682218e-03  7.07503662e-03\n",
      "   1.82281576e-02]\n",
      " [ 4.30769029e-03  1.54272963e-02 -9.00721171e-03 -1.37125010e-03\n",
      "   1.29757901e-02]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.00675271  0.00031958  0.00918146  0.00380509]\n",
      " [ 0.00516367 -0.00355239  0.00208777  0.00328411]\n",
      " [-0.00498225 -0.02091777 -0.00082588  0.02455183]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "param = param_I_L([5,4,3], \"rand\", 9)\n",
    "print(\"Inicializacion aleatoria:\")\n",
    "print(\"W1 = \" + str(param[\"W1\"]))\n",
    "print(\"b1 = \" + str(param[\"b1\"]))\n",
    "print(\"W2 = \" + str(param[\"W2\"]))\n",
    "print(\"b2 = \" + str(param[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "       \n",
    "Inicializacion aleatoria:\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> W1 </td>\n",
    "    <td>[[ 1.10855471e-05 -2.89544069e-03 -1.11606630e-02 -1.28827567e-04\n",
    "  -3.78361464e-03]\n",
    " [-4.81135363e-03 -1.51733118e-02 -4.90871981e-03 -2.40680579e-03\n",
    "  -6.47947460e-03]\n",
    " [ 6.35891080e-03  1.74011731e-02  2.96682218e-03  7.07503662e-03\n",
    "   1.82281576e-02]\n",
    " [ 4.30769029e-03  1.54272963e-02 -9.00721171e-03 -1.37125010e-03\n",
    "   1.29757901e-02]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b1 </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W2 </td>\n",
    "    <td>[[ 0.00675271  0.00031958  0.00918146  0.00380509]\n",
    " [ 0.00516367 -0.00355239  0.00208777  0.00328411]\n",
    " [-0.00498225 -0.02091777 -0.00082588  0.02455183]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b2 </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializacion de Xavier:\n",
      "W1 = [[ 5.54277356e-04 -1.44772035e-01 -5.58033152e-01 -6.44137837e-03\n",
      "  -1.89180732e-01]\n",
      " [-2.40567681e-01 -7.58665589e-01 -2.45435991e-01 -1.20340289e-01\n",
      "  -3.23973730e-01]\n",
      " [ 3.17945540e-01  8.70058653e-01  1.48341109e-01  3.53751831e-01\n",
      "   9.11407882e-01]\n",
      " [ 2.15384515e-01  7.71364813e-01 -4.50360586e-01 -6.85625052e-02\n",
      "   6.48789506e-01]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.38986799  0.01845103  0.53009178  0.21968724]\n",
      " [ 0.29812491 -0.2050976   0.12053746  0.18960822]\n",
      " [-0.2876502  -1.20768788 -0.04768205  1.41750037]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "param = param_I_L([5,4,3], \"Xav\", 9)\n",
    "print(\"Inicializacion de Xavier:\")\n",
    "print(\"W1 = \" + str(param[\"W1\"]))\n",
    "print(\"b1 = \" + str(param[\"b1\"]))\n",
    "print(\"W2 = \" + str(param[\"W2\"]))\n",
    "print(\"b2 = \" + str(param[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "Inicializacion de Xavier:\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 5.54277356e-04 -1.44772035e-01 -5.58033152e-01 -6.44137837e-03\n",
    "  -1.89180732e-01]\n",
    " [-2.40567681e-01 -7.58665589e-01 -2.45435991e-01 -1.20340289e-01\n",
    "  -3.23973730e-01]\n",
    " [ 3.17945540e-01  8.70058653e-01  1.48341109e-01  3.53751831e-01\n",
    "   9.11407882e-01]\n",
    " [ 2.15384515e-01  7.71364813e-01 -4.50360586e-01 -6.85625052e-02\n",
    "   6.48789506e-01]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[ 0.38986799  0.01845103  0.53009178  0.21968724]\n",
    " [ 0.29812491 -0.2050976   0.12053746  0.18960822]\n",
    " [-0.2876502  -1.20768788 -0.04768205  1.41750037]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializacion de He:\n",
      "W1 = [[ 7.01111560e-04 -1.83123748e-01 -7.05862307e-01 -8.14777077e-03\n",
      "  -2.39296801e-01]\n",
      " [-3.04296722e-01 -9.59644498e-01 -3.10454700e-01 -1.52219763e-01\n",
      "  -4.09797956e-01]\n",
      " [ 4.02172831e-01  1.10054682e+00  1.87638310e-01  4.47464605e-01\n",
      "   1.15284991e+00]\n",
      " [ 2.72442256e-01  9.75707886e-01 -5.69666088e-01 -8.67254714e-02\n",
      "   8.20661024e-01]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.47748882  0.0225978   0.64922719  0.26906082]\n",
      " [ 0.36512695 -0.25119223  0.14762763  0.2322217 ]\n",
      " [-0.35229811 -1.47910954 -0.05839835  1.7360763 ]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "param = param_I_L([5,4,3], \"He\", 9)\n",
    "print(\"Inicializacion de He:\")\n",
    "print(\"W1 = \" + str(param[\"W1\"]))\n",
    "print(\"b1 = \" + str(param[\"b1\"]))\n",
    "print(\"W2 = \" + str(param[\"W2\"]))\n",
    "print(\"b2 = \" + str(param[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo discutido en clase respecto a la formulación del ejercicio, el número de capas para inicializar el \"He\" se deja en L-1, por lo tanto la salida esperada cambia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "Inicializacion de He:\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> W1 </td>\n",
    "    <td>[[ 7.83866554e-04 -2.04738575e-01 -7.89178051e-01 -9.10948466e-03\n",
    "  -2.67541957e-01]\n",
    " [-3.40214078e-01 -1.07291517e+00 -3.47098907e-01 -1.70186869e-01\n",
    "  -4.58168043e-01]\n",
    " [ 4.49642895e-01  1.23044875e+00  2.09786008e-01  5.00280637e-01\n",
    "   1.28892539e+00]\n",
    " [ 3.04599702e-01  1.09087458e+00 -6.36906048e-01 -9.69620247e-02\n",
    "   9.17526919e-01]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b1 </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W2 </td>\n",
    "    <td>[[ 0.5513566   0.02609369  0.74966299  0.31068468]\n",
    " [ 0.42161229 -0.2900518   0.17046571  0.26814652]\n",
    " [-0.40679882 -1.70792858 -0.06743261  2.00464824]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b2 </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Propagación hacia delante\n",
    "\n",
    "Una vez inicializados los parámetros, debemos implementar la propagación hacia delante. Vamos a empezar por implementar algunas funciones básicas para ser utilizadas más adelante en la implementación del modelo. \n",
    "\n",
    "Vamos a implementar 3 funciones:\n",
    "- LINEAL\n",
    "- LINEAL -> ACTIVACION donde la activación será ReLU o Sigmoide. \n",
    "- [LINEAL -> RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE (modelo completo)\n",
    "\n",
    "Esta implementación (vectorizada) de la propagación hacia delante calcula las siguientes ecuaciones:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n",
    "\n",
    "donde $A^{[0]} = X$. \n",
    "\n",
    "### Ejercicio 2.1 \n",
    "Construya la parte LINEAL de la propagación hacia delante. \n",
    "\n",
    "*Ayuda:* Puede ser útil la función`np.dot()`. También, si las dimensiones no casan, puede investigar lo que ocurre llamando a `W.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineal(A, W, b):\n",
    "    \"\"\"\n",
    "    Implemente la parte lineal para la propagación hacia delante de una capa\n",
    "    Input:\n",
    "    A: las activaciones de la capa previa (o los datos de entrada): (tamaño de la capa previa, número de ejemplos)\n",
    "    W: matriz de pesos, un arreglo numpy de dimensiones (tamaño de la capa actual, tamaño de la capa previa)\n",
    "    b: vector de sesgo, un arreglo numpy de dimensiones (tamaño de la capa actual, 1)\n",
    "    Output:\n",
    "    Z: la entrada para la función de activación, también llamado parámetro de pre-activación \n",
    "    memo: diccionario python con \"A\", \"W\" y \"b\", almacenados para computar los pasos hacia atrás de manera eficiente\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A)+b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    memo = (A, W, b)\n",
    "    \n",
    "    return Z, memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[-0.01071957 -0.08648949]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(9)\n",
    "A = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "\n",
    "Z, memo = lineal(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table style=\"width:35%\"> \n",
    "  <tr>\n",
    "    <td> Z </td>\n",
    "    <td> [[-0.01071957 -0.08648949]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activación lineal\n",
    "\n",
    "Ahora construyamos la *activación-lineal hacia delante*. Para ello, necesitamos las dos funciones de activación:\n",
    "\n",
    "- **Sigmoide**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. Esta función `sigmoide` devuelve 2 objetos: el valor de activación \"`a`\" y una memoria \"`memo`\" que contiene \"`Z`\" (la cual se le pasa a la correspondiente función de retro-propagación). \n",
    "\n",
    "Para usarla basta con este comando: \n",
    "``` python\n",
    "A, memo = sigmoide(Z)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(Z):\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    memo = Z\n",
    "    \n",
    "    return A, memo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ReLU**:  $A = RELU(Z) = max(0, Z)$. Esta función `relu` también devuelve 2 objetos: el valor de activación \"`a`\" y una \"`memo`\" que contiene \"`Z`\" (la cual se le pasa a la correspondiente función de retro-propagación). \n",
    "\n",
    "Para usarla basta con este comando: \n",
    "``` python\n",
    "A, memo = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    memo = Z \n",
    "    return A, memo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mayor conveniencia, vamos a agrupar dos funciones (Lineal y Activacion) en una sola (LINEAL->ACTIVACION). Por lo tanto, vamos a implementar una función que da el paso LINEAL hacia delante seguido del paso de ACTIVACION hacia delante.\n",
    "\n",
    "### Ejercicio 2.2  \n",
    "\n",
    "Implemente la propagación hacia delante de la capa *LINEAL->ACTIVACION*. La ecuación matemática es: $$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$$ donde la activación \"g\" puede ser sigmoide() o relu(). Utilice la función que acabamos de escribir `lineal()` y la función de activación correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activacion_lineal(A_prev, W, b, activacion):\n",
    "    \"\"\"\n",
    "    Implementa la activación lineal para una capa LINEAL->ACTIVACION\n",
    "    Input:\n",
    "    A_prev: activaciones de la capa previa: (tamaño de la capa previa, número de ejemplos)\n",
    "    W: matriz de pesos, un arreglo numpy de dimensiones (tamaño de la capa actual, tamaño de la capa previa)\n",
    "    b: vector de sesgo, un arreglo numpy de dimensiones (tamaño de la capa actual, 1)\n",
    "    activacion: la activación a ser usada en la capa, guardada como una cadena de texto: \"sigmoide\" or \"relu\"\n",
    "    Output:\n",
    "    A: la salida de la función de activación, también llamada valor de post-activacion \n",
    "    memo: dicionario python con la \"memo_lineal\" y la \"memo_activacion\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if (activacion == \"sigmoide\"):\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, memo_activacion\"\n",
    "        Z, memo_lineal = lineal(A_prev,W,b)\n",
    "        A, memo_activacion = sigmoide(Z)\n",
    "    \n",
    "    elif (activacion == \"relu\"):\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, memo_activacion\".\n",
    "        Z, memo_lineal = lineal(A_prev,W,b)\n",
    "        A, memo_activacion = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    memo = (memo_lineal, memo_activacion)\n",
    "\n",
    "    return A, memo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con sigmoide: A = [[0.09761704 0.57236238]]\n",
      "Con ReLU: A = [[0.         0.29149618]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(99)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "\n",
    "A, activacion_lineal_memo = activacion_lineal(A_prev, W, b, activacion=\"sigmoide\")\n",
    "print(\"Con sigmoide: A = \" + str(A))\n",
    "\n",
    "A, activacion_lineal_memo = activacion_lineal(A_prev, W, b, activacion=\"relu\")\n",
    "print(\"Con ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> Con sigmoide: A  </td>\n",
    "    <td > [[0.09761704 0.57236238]] </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Con ReLU: A </td>\n",
    "    <td > [[0.         0.29149618]] </td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con L capas \n",
    "\n",
    "Con el fin de facilitar la implementación de la red neuronal de $L$ capas  necesitamos una función que replique la propagación hacia delante (`activacion_lineal`) con RELU, $L-1$ veces, seguida por la función (`activacion_lineal`) con la función SIGMOIDE.\n",
    "\n",
    "\n",
    "### Ejercicio 2.3\n",
    "Implemente la propagación hacia delante del modelo completo.\n",
    "\n",
    "Tenga en cuenta que en el código la variable `AL` denota la estimación de la última capa de salida $$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$$ \n",
    "\n",
    "Utilice las funciones que ha programado antes, recuerde usar un bucle for para replicar la [LINEAL->RELU] (L-1) veces y no olvide ir guardando las salidas de cada capa en la lista \"memos\". \n",
    "\n",
    "*Ayuda:* Para ir guardando los valores de `c`en una lista use `list.append(c)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagacion_L(X, param):\n",
    "    \"\"\"\n",
    "    Implemente la propagación hacia delante para calcular [LINEAL->RELU]*(L-1)->LINEAL->SIGMOIDE\n",
    "    Input:\n",
    "    X: datos de entrada, arreglo de tamaño (tamaño del input, número de ejemplos)\n",
    "    param: parametros que se obtienen de la inicializacion param_I_L()\n",
    "    Output:\n",
    "    AL: último valor de post-activación\n",
    "    memos: lista de memos con cada memo de activacion_lineal() (hay L-1 memos, indexadas de 0 a L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    memos = []\n",
    "    A = X\n",
    "    L = len(param) // 2                  # número de capas en la red neuronal\n",
    "    \n",
    "    # Implemente [LINEAL -> RELU]*(L-1). Añada \"memo\" a la lista de \"memos\".\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, memo = activacion_lineal(A_prev,param[\"W\"+str(l)],param[\"b\"+str(l)],\"relu\")\n",
    "        memos.append(memo)\n",
    "\n",
    "    \n",
    "    # Implemente LINEAL -> SIGMOIDE. Añada \"memo\" a la lista de \"memos\".\n",
    "    AL, memo = activacion_lineal(A,param[\"W\"+str(L)],param[\"b\"+str(L)],\"sigmoide\")\n",
    "    memos.append(memo)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, memos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.40900153 0.40900153]]\n",
      "Longitud de la lista de memos = 2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(55)\n",
    "\n",
    "X = np.random.randn(4,2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "\n",
    "params = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "AL, memos = propagacion_L(X, params)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Longitud de la lista de memos = \" + str(len(memos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> AL </td>\n",
    "    <td > [[0.40900153 0.40900153]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Longitud de la lista de memos  </td>\n",
    "    <td > 2 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien, llegado a este punto ya tenemos listo todo el proceso de propagación hacia delante. Este proceso toma la entrada X y obtiene como salida el vector-fila $A^{[L]}$ de predicciones. Y hemos guardado los valores intermedios en \"`memos`\". A partir de aquí podemos calcular el coste o pérdida de las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Función de pérdida o coste\n",
    "\n",
    "Ahora, con el fin de implementar la propagación hacia delante y hacia atrás, debemos computar el coste con el fin de verificar si el modelo en verdad está aprendiendo.\n",
    "\n",
    "### Ejercicio 3.1\n",
    "Calcule el coste por entropía-cruzada $J$, en base a la siguiente fórmula: $$J=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\biggr( y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\biggl) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perdida_L(AL, Y):\n",
    "    \"\"\"\n",
    "    Implementa la función de coste por entropía cruzada.\n",
    "    Input:\n",
    "    AL: vector con las probabilidades para las etiquetas de predicción, dimensiones (1, número de ejemplos)\n",
    "    Y: vector de etiquetas observadas, de dimensión (1, número de ejemplos)\n",
    "    Output:\n",
    "    coste: coste de entropía cruzada\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute la pérdida de AL e Y.\n",
    "    coste = np.sum((-1/m)*(np.multiply(np.log(AL),Y)+((1-Y)*np.log(1-AL))))\n",
    "    \n",
    "    coste = np.squeeze(coste)      # Para asegurar que la dimensión de se coste es correcta (e.g. [[17]] se torna en 17).\n",
    "    assert(coste.shape == ())\n",
    "    \n",
    "    return coste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coste = 0.3225280087539018\n"
     ]
    }
   ],
   "source": [
    "Y = np.asarray([[1, 0, 1]])\n",
    "AL = np.array([[.8,.5,0.95]])\n",
    "\n",
    "print(\"coste = \" + str(perdida_L(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td>coste </td>\n",
    "    <td> 0.32252800875390186 </td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retro-propagación\n",
    "\n",
    "Como en el caso de la propagación hacia delante, vamos a implementar funciones auxiliares para la retro-propagación. Recuerde que la retro-propagación permite calcular el gradiente de la función de coste con respecto a los parámetros. \n",
    "\n",
    "Análogamente a la propagación hacia delante, la retro-propagación se va a construir en tres pasos:\n",
    "- LINEAL hacia atrás\n",
    "- LINEAL -> ACTIVACION hacia atrás, donde ACTIVACION calcula la derivada de la función de activación (ReLU o sigmoide)\n",
    "- [LINEAL -> RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE hacia atrás (modelo completo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La retro-propagación, o propagación lineal hacia atrás para la capa $l$, se calcula a partir de la derivada $$dZ^{[l]} = \\frac{\\partial J }{\\partial Z^{[l]}}$$. \n",
    "\n",
    "Ahora queremos obtener $(dW^{[l]}, db^{[l]} dA^{[l-1]})$ para poder actualizar nuestros parametros. Estos tres gradientes $(dW^{[l]}, db^{[l]}, dA^{[l]})$ se calculan a partir de $dZ^{[l]}$. \n",
    "\n",
    "Estas son la fórmulas que se necesitan:\n",
    "$$ dW^{[l]} = \\frac{\\partial J }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial J }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial J }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4.1\n",
    "\n",
    "Utilice las tres fórmulas (arriba) para implementar `retro_lineal()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retro_lineal(dZ, memo):\n",
    "    \"\"\"\n",
    "    Implementa la parte lineal de la retro-propagación para una sola capa [l]\n",
    "    Input:\n",
    "    dZ: Gradiente del coste con respecto al output lineal de la capa actual\n",
    "    memo: conjunto de valores (A_prev, W, b) provenientes de la propagación hacia delante en la capa actual\n",
    "    Output:\n",
    "    dA_prev: Gradiente del coste con respecto a la activación (de la capa previa: l-1), del mismo tamaño como A_prev\n",
    "    dW: Gradiente del coste con respecto a W (de la capa actual: l), del mismo tamaño que W\n",
    "    db: Gradiente del coste con respecto a b (de la capa actual: l), del mismo tamaño que b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = memo\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1/m*np.dot(dZ,A_prev.T)\n",
    "    db = 1/m*(np.sum(dZ,axis = 1,keepdims=True)) #axis=1,keepdims=True\n",
    "    dA_prev =np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.32506047 -1.14728627]\n",
      " [-0.11305387  0.39901854]\n",
      " [-0.08917309  0.31473239]]\n",
      "dW = [[ 0.19426711  0.29803996 -0.84899749]]\n",
      "db = [[-0.32554075]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "dZ = np.random.randn(1,2)\n",
    "A = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "activacion_lineal_memo = (A, W, b)\n",
    "\n",
    "dA_prev, dW, db = retro_lineal(dZ, activacion_lineal_memo)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table style=\"width:90%\">\n",
    "    <tr>\n",
    "      <td> dA_prev </td>\n",
    "      <td > [[ 0.32506047 -1.14728627]\n",
    " [-0.11305387  0.39901854]\n",
    " [-0.08917309  0.31473239]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td> dW </td>\n",
    "        <td > [[ 0.19426711  0.29803996 -0.84899749]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td> db </td>\n",
    "        <td> [[-0.32554075]] </td> \n",
    "    </tr> \n",
    "    \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activación-lineal hacia atrás\n",
    "\n",
    "A continuación, vamos a escribir una función que da el paso hacia atrás de la activación **`retro_activacion_lineal`**. \n",
    "\n",
    "Para implementar `retro_activacion_lineal`, necesitamos calcular las derivadas de la funcion sigmoide:\n",
    "- **`sigmoide_retro`**, tal que \n",
    "\n",
    "```python\n",
    "dZ = sigmoide_retro(dA, memo_activacion)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide_retro(dA, memo):\n",
    "    \n",
    "    Z = memo\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y la derivada de la funcion ReLU:\n",
    "- **`relu_retro`**, de forma que \n",
    "\n",
    "```python\n",
    "dZ = relu_retro(dA, memo_activacion)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_retro(dA, memo):\n",
    "    \n",
    "    Z = memo\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a implementar una función que da el paso LINEAL hacia atrás seguido del paso de ACTIVACION hacia atrás, todo en una misma capa *RETRO(LINEAL->ACTIVACION)*.\n",
    "\n",
    "### Ejercicio 4.2\n",
    "\n",
    "Si $g(.)$ es la función de activación, entonces \n",
    "`sigmoide_retro` y `relu_retro` calculan $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retro_activacion_lineal(dA, memo, activacion):\n",
    "    \"\"\"\n",
    "    Implementa la retro-propagación para la capa LINEAL->ACTIVACION .\n",
    "    Input:\n",
    "    dA: gradiente post-activacion para la capa actual l \n",
    "    memo: conjunto de valores (linear_cache, activation_cache) que se guardan para calcular la retro-propagación de manera eficiente\n",
    "    activacion: la activación a ser usada en esta capa, guardada como un arreglo de texto: \"sigmoid\" o \"relu\"\n",
    "    Output:\n",
    "    dA_prev: gradiente del coste con respecto a la activación (de la capa previa l-1), de las mismas dimensiones que A_prev\n",
    "    dW: gradiente del coste con respecto a W (capa actual l), mismas dimensiones que W\n",
    "    db: gradiente del coste con respecto a b (capa actual l), mismas dimensiones que b\n",
    "    \"\"\"\n",
    "    memo_lineal, memo_activacion = memo\n",
    "    \n",
    "    if activacion == \"relu\":\n",
    "        dZ = relu_retro(dA,memo_activacion)\n",
    "        dA_prev, dW, db = retro_lineal(dZ, memo_lineal)\n",
    "        \n",
    "    elif activacion == \"sigmoide\":\n",
    "        dZ = sigmoide_retro (dA,memo_activacion)\n",
    "        dA_prev, dW, db = retro_lineal(dZ, memo_lineal)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoide:\n",
      "dA_prev = [[ 0.40376792 -0.11653974]\n",
      " [ 0.23825493 -0.06876764]\n",
      " [-0.24814639  0.07162262]]\n",
      "dW = [[-0.016823   -0.15662557  0.10029677]]\n",
      "db = [[0.14112215]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 1.72037702  0.        ]\n",
      " [ 1.01515818  0.        ]\n",
      " [-1.05730376 -0.        ]]\n",
      "dW = [[ 0.02774167 -0.66684733 -0.00075261]]\n",
      "db = [[0.84526285]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "dAL = np.random.randn(1,2)\n",
    "A = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "Z = np.random.randn(1,2)\n",
    "memo_lineal = (A, W, b)\n",
    "memo_activacion = (memo_lineal, Z)\n",
    "\n",
    "dA_prev, dW, db = retro_activacion_lineal(dAL, memo_activacion, \"sigmoide\")\n",
    "print (\"sigmoide:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = retro_activacion_lineal(dAL, memo_activacion, \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada con el sigmoide:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "      <td > dA_prev </td> \n",
    "      <td >[[ 0.40376792 -0.11653974]\n",
    " [ 0.23825493 -0.06876764]\n",
    " [-0.24814639  0.07162262]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "      <td > dW </td> \n",
    "      <td > [[-0.016823   -0.15662557  0.10029677]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "      <td > db </td> \n",
    "      <td > [[0.14112215]] </td> \n",
    "    </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada con RELU:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "      <td > dA_prev </td> \n",
    "      <td > [[ 1.72037702  0.        ]\n",
    " [ 1.01515818  0.        ]\n",
    " [-1.05730376  0.        ]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "      <td > dW </td> \n",
    "      <td > [[ 0.02774167 -0.66684733 -0.00075261]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "      <td > db </td> \n",
    "      <td > [[0.84526285]] </td> \n",
    "    </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retro-propagación en L capas \n",
    "\n",
    "Ahora vamos a implementar la función de retro-propagación para toda la red neuronal. Recordemos que cuando implementamos la función `propagacion_L`, guardamos en cada iteración una memoria que contenía (X, W, b, z). En el paso hacia atrás de la retro-propagación, esas variables están a disposición para calcular los gradientes. Por lo tanto, en la función `retro_L`, se puede iterar sobre todas las capas escondidas hacia atrás, empezando desde la última capa $L$. \n",
    "\n",
    "En cada paso hacia atrás, se utilizan los valores de la memoria en la capa $l$, para retro-propagar sobre la capa $l$. \n",
    "\n",
    "Tenga en cuenta que para retro-propagar sobre esta red, sabemos que la salida es, \n",
    "$$A^{[L]} = \\sigma(Z^{[L]})$$. Por lo tanto, necesitamos calcular `dAL` $= \\frac{\\partial J}{\\partial A^{[L]}}$\n",
    "\n",
    "Para hacerlo, utilizamos la fórmula:\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivada del coste con respecto a AL\n",
    "```\n",
    "\n",
    "Luego utilizamos el gradiente de post-activacion `dAL` para seguir yendo hacia atrás. Este gradiente se le puede pasar a la función RETRO(LINEAL->SIGMOIDE) implementada antes (que utiliza los valores guaradados por la función `propagacion_L`). \n",
    "\n",
    "Entonces implementamos un bucle `for` para iterar sobre todas las otras capas utilizando la función hacia atrás RETRO(LINEAL->RELU). \n",
    "\n",
    "*Nota:* Debemos guardar cada dA, dW, y db en un diccionario `grads`. \n",
    "Para hacerlo, utilizamos la fórmula : \n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}$$\n",
    "\n",
    "Por ejemplo, para $l=3$ se guardaría $dW^{[l]}$ en `grads[\"dW3\"]`.\n",
    "\n",
    "### Ejercicio 4.3\n",
    "Implemente la retro-propagación para el modelo [LINEAL->RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retro_L(AL, Y, memos):\n",
    "    \"\"\"\n",
    "    Implementa la retro-propagación para  [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    Input:\n",
    "    AL: vector con las probabilidades, salida para propagación hacia delante propagacion_L()\n",
    "    Y: vector de clases/etiquetas observadas, de dimensión (1, número de ejemplos)\n",
    "    memos: lista de memos, donde se tiene\n",
    "                - cada memo de activacion_lineal() con \"relu\" (i.e., memos[l]; l = 0...L-2)\n",
    "                - la memo de activacion_lineal() con \"sigmoide\" (i.e, [L-1])\n",
    "    Output:\n",
    "    grads: Un diccionario con los gradientes\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(memos) # número de capas\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # Y es del mismo tamaño que AL\n",
    "    \n",
    "    # Inicializacion de la retro-propagación\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))   # derivada del coste con respecto a AL\n",
    "    \n",
    "    # Gradientes para la ultima capa L (SIGMOIDE -> LINEAL). \n",
    "    # Inputs: \"dAL, memo_actual\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    memo_actual = memos[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = retro_activacion_lineal(dAL,memo_actual,\"sigmoide\")\n",
    "    \n",
    "    #Bucle de l=L-2 a l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        memo_actual = memos[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = retro_activacion_lineal(grads[\"dA\" + str(l + 1)], memo_actual, \"relu\") \n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[-0.0040954   0.02626375  0.0120443   0.04524661]\n",
      " [-0.00982808  0.06302743  0.02890377  0.10858225]\n",
      " [ 0.00759414  0.01970338  0.03535496  0.0671214 ]]\n",
      "db1 = [[-0.02157541]\n",
      " [-0.0517764 ]\n",
      " [-0.05506261]]\n",
      "dA1 = [[ 0.04201136 -0.08516219]\n",
      " [ 0.10081834 -0.20437115]\n",
      " [ 0.05432588 -0.11012523]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(333)\n",
    "AL = np.random.randn(1, 2)\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "A1 = np.random.randn(4,2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "Z1 = np.random.randn(3,2)\n",
    "memo_lin1 = ((A1, W1, b1), Z1)\n",
    "\n",
    "A2 = np.random.randn(3,2)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "Z2 = np.random.randn(1,2)\n",
    "memo_lin2 = ((A2, W2, b2), Z2)\n",
    "\n",
    "memos = (memo_lin1, memo_lin2)\n",
    "    \n",
    "grads = retro_L(AL, Y, memos)\n",
    "\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "    <tr>\n",
    "      <td > dW1 </td> \n",
    "      <td > [[-0.0040954   0.02626375  0.0120443   0.04524661]\n",
    " [-0.00982808  0.06302743  0.02890377  0.10858225]\n",
    " [ 0.00759414  0.01970338  0.03535496  0.0671214 ]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "      <td > db1 </td> \n",
    "      <td > [[-0.02157541]\n",
    " [-0.0517764 ]\n",
    " [-0.05506261]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "      <td > dA1 </td> \n",
    "      <td > [[ 0.04201136 -0.08516219]\n",
    " [ 0.10081834 -0.20437115]\n",
    " [ 0.05432588 -0.11012523]] </td> \n",
    "    </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Actualización de parámetros\n",
    "\n",
    "En este paso se actualizan los parámetros del modelo, utilizando el método de descenso en la dirección del gradiente (GD): \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
    "\n",
    "donde $\\alpha$ es la tasa de aprendizaje. Tras la actualización de los parametros, los guardamos en un diccionario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5.1\n",
    "Implemente `rev_param()` para actualizar los parámtros usando GD en cada $W^{[l]}$ y $b^{[l]}$; $l = 1, 2, ..., L$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_param(param, grads, tasa):\n",
    "    \"\"\"\n",
    "    Actualice los parametros utilizando GD\n",
    "    Input: \n",
    "    param: diccionario python con los parametros \n",
    "    grads: diccionario python con los gradientes, resultado de retro_L\n",
    "    Output:\n",
    "    param: diccionario python con los parametros actualizados \n",
    "                  param[\"W\" + str(l)] = ... \n",
    "                  param[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(param) // 2 # numero de capas en la red neuronal\n",
    "\n",
    "    # Regla de actualización para cada parámetro (utilice on bucle for).\n",
    "    for l in range(L):\n",
    "        param[\"W\" + str(l+1)] = param[\"W\" + str(l+1)] - tasa*grads[\"dW\" + str(l+1)]\n",
    "        param[\"b\" + str(l+1)] = param[\"b\" + str(l+1)] - tasa*grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "params = {\"W1\": W1,\"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "np.random.seed(3)\n",
    "dW1 = np.random.randn(3,4)\n",
    "db1 = np.random.randn(3,1)\n",
    "dW2 = np.random.randn(1,3)\n",
    "db2 = np.random.randn(1,1)\n",
    "grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "param = rev_param(params, grads, tasa=0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(param[\"W1\"]))\n",
    "print (\"b1 = \"+ str(param[\"b1\"]))\n",
    "print (\"W2 = \"+ str(param[\"W2\"]))\n",
    "print (\"b2 = \"+ str(param[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "      <td > W1 </td> \n",
    "      <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "      <td > b1 </td> \n",
    "      <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "      <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "      <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "    </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Modelo de L capas\n",
    "\n",
    "Ahora combinemos las distintas funciones para contruir nuestra red neuronal de $L$ capas.\n",
    "\n",
    "Esta función `modelo_red_L` llama a las funciones auxiliares que hemos implementado arriba para construir una red de L capas con la siguiente estructura: [LINEAL -> RELU]$\\times$(L-1) -> LINEAL -> SIGMOIDE. \n",
    "\n",
    "Las funciones que vamos a utilizar son:\n",
    "\n",
    "```python\n",
    "def param_I_L(dim_capas, init, semilla):\n",
    "    ...\n",
    "    return param \n",
    "def propagacion_L(X, param):\n",
    "    ...\n",
    "    return AL, memos\n",
    "def perdida_L(AL, Y):\n",
    "    ...\n",
    "    return coste\n",
    "def retro_L(AL, Y, memo):\n",
    "    ...\n",
    "    return grads\n",
    "def rev_param(param, grads, tasa):\n",
    "    ...\n",
    "    return param\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_red_L(X, Y, xV, yV, dim_capas, tasa, num_iter, init, semilla, print_c):\n",
    "    \"\"\"\n",
    "    Implementa una red neuronal de L capas: [LINEAL->RELU]*(L-2)->LINEAL->SIGMOIDE\n",
    "    Input:\n",
    "    X: datos de entrada de tamaño (n_x, número de ejemplos)\n",
    "    Y: vector de etiquetas observadas de tamaño (1, número de ejemplos)\n",
    "    xV: dataos de entrada de validacion\n",
    "    yV: etiquetas de validación\n",
    "    dim_capas: dimensiones de las capas (n_x, n_h, n_y)\n",
    "    tasa: tasa de aprendizaje de la regla de actualización por GD \n",
    "    num_iter: número de iteraciones del bucle de optimización\n",
    "    init: método de inicializacion de parametros\n",
    "    semilla: semilla aleatoria para la inciializacion de parametros\n",
    "    print_c: si es verdadero \"True\", muestra el coste cada 100 iteraciones \n",
    "    Output:\n",
    "    param: diccionario con los parametros\n",
    "    \"\"\"\n",
    "\n",
    "    costes = []   \n",
    "    costes2 = [] \n",
    "    coste_aux= 99\n",
    "    epoc=0\n",
    "    \n",
    "    # Inicialización de parámetros \n",
    "    param = param_I_L(dim_capas, init, semilla)\n",
    "    \n",
    "    # Bucle (GD)\n",
    "    for i in range(0, num_iter):\n",
    "\n",
    "        # Propagación hacia delante: [LINEAL -> RELU]*(L-2) -> LINEAL -> SIGMOIDE\n",
    "        AL, memo = propagacion_L(X, param)\n",
    "        \n",
    "        # Calcule el coste\n",
    "        coste = perdida_L(AL, Y)\n",
    "        \n",
    "        # coste de validacion\n",
    "        AL2,memo2 =  propagacion_L(xV, param)  # propagacion de validacion\n",
    "        coste2 = perdida_L(AL2, yV)           # coste de validacion\n",
    "        \n",
    "        # Retro-propagación\n",
    "        grads = retro_L(AL, Y, memo)\n",
    " \n",
    "        # Actualize los parámetros\n",
    "        param = rev_param(param, grads, tasa)\n",
    "        \n",
    "        # Me quedo con la mejor época en validacion\n",
    "        if coste2 < coste_aux:\n",
    "            coste_aux=coste2\n",
    "            epoc = i+1\n",
    "                \n",
    "        # Imprime el coste cada 100 iteraciones\n",
    "        if print_c and i % 100 == 0:\n",
    "            print (\"Coste tras la iteración %i: %f\" %(i, coste))\n",
    "        if print_c and i % 100 == 0:\n",
    "            costes.append(coste)\n",
    "            costes2.append(coste2)\n",
    "            \n",
    "    # grafique el coste\n",
    "    if print_c:\n",
    "        plt.plot(np.squeeze(costes), marker=\"o\", label=\"CE\")\n",
    "        plt.plot(np.squeeze(costes2), marker=\"*\", label=\"CV\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.ylabel('Coste')\n",
    "        plt.xlabel('Epocas')\n",
    "        plt.title(\"Tasa de aprendizaje= \" + str(tasa))\n",
    "        #plt.savefig(\"fig2\"+str(i)+\".png\")\n",
    "        plt.show()\n",
    "        print(\"Mejor iteracion es: \" +str(epoc) +\" con la semilla: \" +str(semilla))\n",
    "    \n",
    "    return param, epoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Caso aplicado\n",
    "\n",
    "Ahora probemos nuestro modelo con las imagenes del paramo. \n",
    "\n",
    "A continuacion carguemos nuestros conjuntos de entrenamiento y validacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = import_imagenes()\n",
    "CE_x, CV_x, CE_y, CV_y = particion_CE_CV(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14700, 175), (14700, 75))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE_x.shape, CV_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que el código funciona, por ejemplo con la siguiente estructura [14700, 700, 100, 50, 15, 1]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 0: 5.581463\n",
      "Coste tras la iteración 100: 0.362460\n",
      "Coste tras la iteración 200: 0.456101\n",
      "Coste tras la iteración 300: 0.198732\n",
      "Coste tras la iteración 400: 0.199771\n",
      "Coste tras la iteración 500: 0.091285\n",
      "Coste tras la iteración 600: 0.080668\n",
      "Coste tras la iteración 700: 0.040423\n",
      "Coste tras la iteración 800: 0.050451\n",
      "Coste tras la iteración 900: 0.019939\n",
      "Coste tras la iteración 1000: 0.012641\n",
      "Coste tras la iteración 1100: 0.009958\n",
      "Coste tras la iteración 1200: 0.008408\n",
      "Coste tras la iteración 1300: 0.007155\n",
      "Coste tras la iteración 1400: 0.006279\n",
      "Coste tras la iteración 1500: 0.005560\n",
      "Coste tras la iteración 1600: 0.004993\n",
      "Coste tras la iteración 1700: 0.004526\n",
      "Coste tras la iteración 1800: 0.004161\n",
      "Coste tras la iteración 1900: 0.003810\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor iteracion es: 314 con la semilla: 66\n"
     ]
    }
   ],
   "source": [
    "dim_capas =[14700, 700, 100, 50, 15, 1]     #  Modelo de 5 capas (Datos de entrada + 4 capas escondidas + capa de salida)\n",
    "\n",
    "paramL, epoc = modelo_red_L(CE_x, CE_y, CV_x, CV_y, dim_capas, tasa = 0.002, num_iter = 2000, init = \"Xav\", semilla = 66, print_c = True) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiquemos el mejor modelo de nuestra red profunda con respecto al numero de epocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in less_equal\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "paramL, epoc = modelo_red_L(CE_x, CE_y, CV_x, CV_y, dim_capas, tasa = 0.002, num_iter = epoc, init = \"He\", semilla = 66, print_c = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos la exactitud del modelo. Primero definimos nuestra funcion de prediccion `pred()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_L(X, param):\n",
    "      \n",
    "    m = X.shape[1]\n",
    "    n = len(param) // 2 \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, memos = propagacion_L(X, param)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y calculamos la exactitud en entrenamiento y validacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La exactitud  en entrenamiento es: 0.582857142857143  y el de validacion es: 0.5733333333333334\n"
     ]
    }
   ],
   "source": [
    "pE=pred_L(CE_x, paramL)\n",
    "pV=pred_L(CV_x, paramL)\n",
    "\n",
    "n=CE_y.shape[1]\n",
    "m=CV_y.shape[1]\n",
    "    \n",
    "# exactitud \n",
    "# entrenamiento\n",
    "Acc_e = np.sum((pE == CE_y)/n)\n",
    "    \n",
    "# validacion\n",
    "Acc_v = np.sum((pV == CV_y)/m)\n",
    "\n",
    "print(\"La exactitud  en entrenamiento es: \" +str(Acc_e), \" y el de validacion es: \" +str(Acc_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el modelo está aprendiendo hasta sobreajustarse a los datos, pero no generaliza bien de acuerdo con su rendimiento sobre los datos de validación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente probemos nuestro mejor modelo sobre la imagen ``IMG_3451.JPG``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import image\n",
    "\n",
    "img = image.load_img(\"images/IMG_3451.JPG\")\n",
    "img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo pasamos por nuestra imagen de prueba\n",
    "x = np.array(img)\n",
    "x2 = x\n",
    "\n",
    "ni = x.shape[0]-50\n",
    "mi = x.shape[1]-50\n",
    "\n",
    "f1=0\n",
    "f2=70\n",
    "for i in range(1,ni,50):\n",
    "    c1=0\n",
    "    c2=70\n",
    "    for j in range(1,mi,50):\n",
    "        subi=x[f1:f2,c1:c2,]\n",
    "        subi2=np.ndarray.flatten(subi).T/255.\n",
    "        ns=subi2.shape[0]\n",
    "        vec=subi2\n",
    "        vec.shape=(ns,1)\n",
    "        pred_P=pred_L(subi2, paramL)\n",
    "        if(pred_P==1):\n",
    "            x2[f1:f2,c1:c2,2]=0\n",
    "        c1=c1+50\n",
    "        c2=c2+50\n",
    "    f1=f1+50\n",
    "    f2=f2+50\n",
    "        \n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados no parecen mejorar significativamente lo conseguidos por  la red sencilla. A continuacion intente mejorar el modelo. Puede intentar con diferentes arquitecturas, cambiando el método de inicialización, la tasa de aprendizaje, aumentando el número de epocas, entre otras estrategias.\n",
    "\n",
    "*Ayuda:* Al explorar distintas estrategias, viene bien analizar qué tipo de errores comete el modelo. Primero analicemos el error de manera manual. Revisemos algunas de las imagenes que el modelo de L capas etiquetó incorrectamente. Ejecute la celda abajo para visualizar algunos de estos errores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clases = [\"NF\", \"F\"]\n",
    "print_errores(clases, CV_x, CV_y, pV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Las distintas imagenes sobre las que el modelo tiende a equivocarse incluyen:** \n",
    "- La planta que se concoce como *puya* y se parece al frailejon \n",
    "- Pastos borrosos que se confunden con frailejon\n",
    "- Frailejones con flor y sombras que no se reconocen\n",
    "- Frailejones borrosos\n",
    "\n",
    "Antes de enfocarse en el modelo (la arquitectura de la red, el método de incialización, regularización, el algoritmo de optimización, etc.), una buena estrategia previa puede consistir en conseguir más ejemplos sobre ese tipo de imagenes en las que el modelo parece equivocarse. Así se puede conseguir que el modelo aprenda a distinguir acertadamente las clases  que más se le están dificultando. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambiamos la proporción de los datos de prueba hasta 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particion_CE_CV(X,Y):\n",
    " \n",
    "    CE_x, CV_x, CE_y, CV_y = train_test_split(X.T, Y.T, test_size = 0.15, random_state = 100)\n",
    " \n",
    "    CE_x = CE_x.T\n",
    "\n",
    "    CV_x = CV_x.T\n",
    "\n",
    "    CE_y = CE_y.T\n",
    "\n",
    "    CV_y = CV_y.T\n",
    " \n",
    "    #n=CE_y.shape[1]\n",
    "\n",
    "     #m=CV_y.shape[1]\n",
    " \n",
    "    #print(CE_x.shape, CV_x.shape, CE_y.shape, CV_y.shape, n, m)\n",
    "\n",
    "    return CE_x, CV_x, CE_y, CV_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para mejorar el rendimiento del modelo intentamos variando las iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 0: 5.581463\n",
      "Coste tras la iteración 100: 0.362460\n",
      "Coste tras la iteración 200: 0.456101\n",
      "Coste tras la iteración 300: 0.198732\n",
      "Coste tras la iteración 400: 0.199771\n",
      "Coste tras la iteración 500: 0.091285\n",
      "Coste tras la iteración 600: 0.080668\n",
      "Coste tras la iteración 700: 0.040423\n",
      "Coste tras la iteración 800: 0.050451\n",
      "Coste tras la iteración 900: 0.019939\n",
      "Coste tras la iteración 1000: 0.012641\n",
      "Coste tras la iteración 1100: 0.009958\n",
      "Coste tras la iteración 1200: 0.008408\n",
      "Coste tras la iteración 1300: 0.007155\n",
      "Coste tras la iteración 1400: 0.006279\n",
      "Coste tras la iteración 1500: 0.005560\n",
      "Coste tras la iteración 1600: 0.004993\n",
      "Coste tras la iteración 1700: 0.004526\n",
      "Coste tras la iteración 1800: 0.004161\n",
      "Coste tras la iteración 1900: 0.003810\n",
      "Coste tras la iteración 2000: 0.003537\n",
      "Coste tras la iteración 2100: 0.003300\n",
      "Coste tras la iteración 2200: 0.003089\n",
      "Coste tras la iteración 2300: 0.002901\n",
      "Coste tras la iteración 2400: 0.002744\n",
      "Coste tras la iteración 2500: 0.002584\n",
      "Coste tras la iteración 2600: 0.002453\n",
      "Coste tras la iteración 2700: 0.002326\n",
      "Coste tras la iteración 2800: 0.002216\n",
      "Coste tras la iteración 2900: 0.002114\n",
      "Coste tras la iteración 3000: 0.002021\n",
      "Coste tras la iteración 3100: 0.001934\n",
      "Coste tras la iteración 3200: 0.001856\n",
      "Coste tras la iteración 3300: 0.001776\n",
      "Coste tras la iteración 3400: 0.001701\n",
      "Coste tras la iteración 3500: 0.001631\n",
      "Coste tras la iteración 3600: 0.001572\n",
      "Coste tras la iteración 3700: 0.001514\n",
      "Coste tras la iteración 3800: 0.001461\n",
      "Coste tras la iteración 3900: 0.001411\n",
      "Coste tras la iteración 4000: 0.001365\n",
      "Coste tras la iteración 4100: 0.001321\n",
      "Coste tras la iteración 4200: 0.001281\n",
      "Coste tras la iteración 4300: 0.001244\n",
      "Coste tras la iteración 4400: 0.001206\n",
      "Coste tras la iteración 4500: 0.001174\n",
      "Coste tras la iteración 4600: 0.001140\n",
      "Coste tras la iteración 4700: 0.001110\n",
      "Coste tras la iteración 4800: 0.001081\n",
      "Coste tras la iteración 4900: 0.001054\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor iteracion es: 314 con la semilla: 66\n"
     ]
    }
   ],
   "source": [
    "dim_capas =[14700, 700, 100, 50, 15, 1]     #  Modelo de 5 capas (Datos de entrada + 4 capas escondidas + capa de salida)\n",
    "\n",
    "paramL, epoc = modelo_red_L(CE_x, CE_y, CV_x, CV_y, dim_capas, tasa = 0.002, num_iter = 5000, init = \"Xav\", semilla = 66, print_c = True) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiquemos el mejor modelo de nuestra red profunda con respecto al numero de epocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paramL, epoc = modelo_red_L(CE_x, CE_y, CV_x, CV_y, dim_capas, tasa = 0.002, num_iter = epoc, init = \"He\", semilla = 66, print_c = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos la exactitud del modelo. Primero definimos nuestra funcion de prediccion `pred()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_L(X, param):\n",
    "      \n",
    "    m = X.shape[1]\n",
    "    n = len(param) // 2 \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, memos = propagacion_L(X, param)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y calculamos la exactitud en entrenamiento y validacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La exactitud  en entrenamiento es: 0.8457142857142858  y el de validacion es: 0.8\n"
     ]
    }
   ],
   "source": [
    "pE=pred_L(CE_x, paramL)\n",
    "pV=pred_L(CV_x, paramL)\n",
    "\n",
    "n=CE_y.shape[1]\n",
    "m=CV_y.shape[1]\n",
    "    \n",
    "# exactitud \n",
    "# entrenamiento\n",
    "Acc_e = np.sum((pE == CE_y)/n)\n",
    "    \n",
    "# validacion\n",
    "Acc_v = np.sum((pV == CV_y)/m)\n",
    "\n",
    "print(\"La exactitud  en entrenamiento es: \" +str(Acc_e), \" y el de validacion es: \" +str(Acc_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentando las iteracciones (epocas) a 5000, se presenta una mejor exactitud en entrenamiento de  0.8457 y en validación de 0.8."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
